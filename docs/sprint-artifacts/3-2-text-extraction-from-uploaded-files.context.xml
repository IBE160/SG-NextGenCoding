<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>2</storyId>
    <title>Text Extraction from Uploaded Files</title>
    <status>drafted</status>
    <generatedAt>2025-12-04</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/3-2-text-extraction-from-uploaded-files.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>As a user,</asA>
    <iWant>I want the system to accurately extract text from my uploaded lecture notes,</iWant>
    <soThat>So that the AI can process the content for summaries and quizzes.</soThat>
    <tasks>
      ### Backend Development

      *   **Research & Select Text Extraction Libraries**
          *   Evaluate Python libraries for PDF parsing (e.g., `PyPDF2`, `pdfminer.six`, `pypdf`).
          *   Evaluate Python libraries for DOCX parsing (e.g., `python-docx`).
          *   Evaluate Python libraries for TXT files.
          *   Select optimal libraries based on accuracy, performance, and ease of integration.
      *   **Implement Text Extraction Service (`backend/app/services/ai_generation/text_extractor.py`)**
          *   Create a module `text_extractor.py` containing functions for:
              *   `extract_text_from_pdf(file_path)`
              *   `extract_text_from_docx(file_path)`
              *   `extract_text_from_txt(file_path)`
          *   Handle different file types dynamically.
          *   Implement error handling for parsing failures (e.g., corrupted files).
      *   **Integrate Text Extraction into Document Processing Flow**
          *   Modify the backend's document processing logic (e.g., in `backend/app/api/summaries/main.py` or a dedicated background task handler) to call the `text_extractor` service after a file upload.
          *   Retrieve the uploaded file from Supabase Storage for extraction.
          *   Store the extracted text in the `documents` table's `raw_content` field or as a reference.
          *   Update the `documents` table `status` to 'text-extracted' on success or 'extraction-failed' on failure.
      *   **Update Database Models (`backend/app/db/models.py`)**
          *   Ensure the `Document` model has a `raw_content` field (TEXT type, potentially nullable if external storage is used for large texts) and a `status` field (TEXT).

      ### Testing

      *   **Unit Tests (Backend `pytest`)**:
          *   Test `extract_text_from_pdf` with various PDF files (simple text, mixed content, image-only).
          *   Test `extract_text_from_docx` with various DOCX files (simple text, basic formatting).
          *   Test `extract_text_from_txt` with sample TXT files.
          *   Test error handling for corrupted or invalid files in each extraction function.
      *   **Integration Tests (Backend `pytest`)**:
          *   Simulate file upload and verify that text extraction is initiated.
          *   Verify that `raw_content` is correctly populated and `status` is updated in the `documents` table after extraction.
          *   Test end-to-end flow from file upload to text extraction for different file types.
      *   **Performance Testing**:
          *   Measure text extraction time for typical and large files to ensure it contributes minimally to the overall summarization threshold.
    </tasks>
  </story>

  <acceptanceCriteria>
    1.  **FR-CIP-2**: When a supported file (PDF, TXT, DOCX) is uploaded and successfully stored (from Story 3.1), the system automatically initiates text extraction.
    2.  **FR-CIP-2**: The text content from TXT files is extracted accurately and completely.
    3.  **FR-CIP-2**: The text content from PDF files is extracted accurately, preserving paragraph breaks and basic formatting where applicable.
    4.  **FR-CIP-2**: The text content from DOCX files is extracted accurately, preserving paragraph breaks and basic formatting where applicable.
    5.  **FR-CIP-2**: The extracted text is securely stored in the `raw_content` field of the corresponding `documents` table entry, or referenced if stored externally.
    6.  **FR-CIP-2**: Upon successful text extraction, the `status` of the `documents` table entry is updated to 'text-extracted' or similar, making the content available for AI processing.
    7.  **FR-CIP-2**: If text extraction fails for any reason (e.g., corrupted file, parsing error), the `status` of the `documents` table entry is updated to 'extraction-failed', and relevant error logs are recorded.
    8.  **FR-CIP-2**: The text extraction process is efficient and does not significantly impact the overall 30-second performance threshold for summary generation for typical file sizes.
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>2. Content Ingestion &amp; Processing (FR-CIP)</section>
        <snippet>FR-CIP-1: The system validates uploaded files for format and size, providing clear error messages for invalid files. The system accurately extracts text content from uploaded PDF, TXT, and DOCX files for AI processing. FR-CIP-2: The system shall generate a concise summary (including key points and a short paragraph) from uploaded lecture notes using AI (Gemini 2.5).</snippet>
      </artifact>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>Edge Cases and Special Scenarios</section>
        <snippet>Empty or Very Short Files: If a user uploads a file with little or no content, the AI should return a message indicating that there is not enough content. Poor Quality OCR: The system should ideally detect this and warn the user that the generated content might be inaccurate.</snippet>
      </artifact>
      <artifact>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Epic to Architecture Mapping</section>
        <snippet>Epic 3: Content Ingestion &amp; AI Summarization -&gt; backend/app/services/ai_generation/.</snippet>
      </artifact>
      <artifact>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Implementation Patterns</section>
        <snippet>Error Recovery: Toast notifications, inline messages with retry. Retry Mechanisms: Exponential backoff for network/external API calls.</snippet>
      </artifact>
      <artifact>
        <path>docs/epics.md</path>
        <title>ibe160 - Epic Breakdown</title>
        <section>Epic 3: Content Ingestion &amp; AI Summarization</section>
        <snippet>Goal: Allow users to upload lecture notes and receive AI-generated summaries. Scope: File upload (PDF, TXT, DOCX), file validation, text extraction, integration with Gemini 2.5 for summarization, displaying the summary in the UI.</snippet>
      </artifact>
    </docs>
    <code>
      <artifact>
        <path>backend/app/services/ai_generation/text_extractor.py</path>
        <kind>Python Service</kind>
        <symbol>extract_text_from_pdf, extract_text_from_docx, extract_text_from_txt</symbol>
        <reason>New module for encapsulating text extraction logic from various file formats.</reason>
      </artifact>
      <artifact>
        <path>backend/app/api/summaries/main.py</path>
        <kind>FastAPI Endpoint</kind>
        <symbol>POST /api/v1/documents/upload</symbol>
        <reason>Modification to integrate text extraction after successful file upload.</reason>
      </artifact>
      <artifact>
        <path>backend/app/db/models.py</path>
        <kind>SQLModel</kind>
        <symbol>Document</symbol>
        <reason>Update Document model to include raw_content (TEXT) and status (TEXT) fields.</reason>
      </artifact>
    </code>
    <dependencies>
      <nodejs>
        <package name="@hookform/resolvers" version="^5.2.2"/>
        <package name="@next/bundle-analyzer" version="^14.0.4"/>
        <package name="@radix-ui/react-dialog" version="^1.1.15"/>
        <package name="@radix-ui/react-dropdown-menu" version="^2.0.6"/>
        <package name="@radix-ui/react-icons" version="^1.3.0"/>
        <package name="@radix-ui/react-label" version="^2.1.8"/>
        <package name="@radix-ui/react-slot" version="^1.0.2"/>
        <package name="@radix-ui/react-toast" version="^1.2.15"/>
        <package name="@supabase/ssr" version="^0.1.0"/>
        <package name="@supabase/supabase-js" version="^2.39.3"/>
        <package name="@tanstack/react-query" version="^5.17.15"/>
        <package name="@tanstack/react-query-devtools" version="^5.17.15"/>
        <package name="@vercel/analytics" version="^1.1.1"/>
        <package name="axios" version="^1.6.5"/>
        <package name="class-variance-authority" version="^0.7.0"/>
        <package name="clsx" version="^2.1.0"/>
        <package name="geist" version="^1.0.0"/>
        <package name="lucide-react" version="^0.304.0"/>
        <package name="next" version="^14.1.0"/>
        <package name="next-themes" version="^0.2.1"/>
        <package name="nextjs-toploader" version="^1.6.4"/>
        <package name="react" version="18.2.0"/>
        <package name="react-dom" version="18.2.0"/>
        <package name="react-hook-form" version="^7.67.0"/>
        <package name="tailwind-merge" version="^2.2.0"/>
        <package name="undici" version="^5.28.2"/>
        <package name="zod" version="^4.1.13"/>
      </nodejs>
      <python>
        <package name="fastapi" version="^0.122.0"/>
        <package name="uvicorn" version="^0.38.0"/>
        <package name="supabase" version="^2.24.0"/>
        <package name="python-dotenv" version="^1.2.1"/>
        <package name="email-validator" version="^2.1.1"/>
        <package name="pypdf" version="^X.X.X"/> (Placeholder, needs research in step 3)
        <package name="python-docx" version="^X.X.X"/> (Placeholder, needs research in step 3)
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Utilize Python libraries such as PyPDF2 (or similar) for PDF parsing and python-docx (or similar) for DOCX parsing.</constraint>
    <constraint>Implement robust error handling for corrupted or unparsable files during extraction.</constraint>
    <constraint>Store extracted raw_content and update status in the 'documents' table.</constraint>
    <constraint>Ensure efficient extraction to meet overall performance thresholds.</constraint>
  </constraints>
  <interfaces>
    <interface>
      <name>Text Extraction Internal Call</name>
      <kind>Internal Function Call</kind>
      <signature>text_extractor.extract_text_from_file(file_path, file_type)</signature>
      <path>backend/app/api/summaries/main.py</path>
    </interface>
    <interface>
      <name>Supabase Storage Download</name>
      <kind>External Service</kind>
      <signature>supabase.storage.from('bucket_name').download('file_path_in_storage')</signature>
      <path>backend/app/api/summaries/main.py</path>
    </interface>

  <tests>
    <standards>Unit and Integration tests are required, focusing on the backend with pytest. Performance testing for text extraction is also a critical standard to ensure efficiency.</standards>
    <locations>
      <location>backend/tests/</location>
    </locations>
    <ideas>
      <idea acId="FR-CIP-2">Test automatic initiation of text extraction after successful file upload and storage.</idea>
      <idea acId="FR-CIP-2">Test accurate and complete text extraction from TXT files.</idea>
      <idea acId="FR-CIP-2">Test accurate text extraction from PDF files, preserving paragraph breaks and basic formatting.</idea>
      <idea acId="FR-CIP-2">Test accurate text extraction from DOCX files, preserving paragraph breaks and basic formatting.</idea>
      <idea acId="FR-CIP-2">Test that extracted text is securely stored in the 'raw_content' field and the 'status' is updated to 'text-extracted' upon success.</idea>
      <idea acId="FR-CIP-2">Test that the 'status' is updated to 'extraction-failed' and relevant error logs are recorded when text extraction fails.</idea>
      <idea acId="FR-CIP-2">Conduct performance tests to measure text extraction time and its impact on the overall summarization threshold for typical and large files.</idea>
    </ideas>
  </tests>
</story-context>