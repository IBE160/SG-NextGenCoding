<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>3</storyId>
    <title>AI Summary Generation Integration</title>
    <status>drafted</status>
    <generatedAt>2025-12-04</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/3-3-ai-summary-generation-integration.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>As a user,</asA>
    <iWant>I want to receive a concise summary of my lecture notes,</iWant>
    <soThat>So that I can quickly grasp the key concepts.</soThat>
    <tasks>
      ### Backend Development

      *   **Implement Gemini API Client (`backend/app/services/ai_generation/gemini_client.py`)**
          *   Create a module for interacting with the Gemini 2.5 API.
          *   Implement function `call_gemini_summarize(prompt: str, text: str)` to send requests and handle responses.
          *   Configure API key retrieval securely (e.g., from environment variables via `backend/app/core/config.py`).
          *   Implement retry logic with exponential backoff for transient API errors/rate limits.
      *   **Develop AI Summary Generation Service (`backend/app/services/ai_generation/summary_generator.py`)**
          *   Create `generate_summary(document_id: UUID, user_id: UUID, extracted_text: str)` function.
          *   Construct an optimized prompt for Gemini 2.5 based on the extracted text.
          *   Call the `gemini_client` to get the summary.
          *   Store the generated summary in the `summaries` table (`backend/app/db/models.py`).
          *   Update the `documents` table `status` (`summarizing`, `summarized`, `summary-failed`).
          *   Implement robust error handling for AI-related failures.
      *   **Integrate Summary Generation into Document Processing Flow**
          *   Modify the backend's document processing logic (e.g., extend the background task from Story 3.2) to trigger `summary_generator.generate_summary` after successful text extraction.
          *   Ensure the `raw_content` from the `documents` table (or external storage) is passed to the summary generator.
      *   **Update Backend API (`backend/app/api/summaries/main.py`)**
          *   Add a new endpoint `POST /api/v1/summaries/generate` (or modify `documents/upload` response to initiate) to allow the frontend to request summary generation for a specific document.
          *   Add an endpoint `GET /api/v1/documents/{document_id}/status` to allow the frontend to query the current status of summary generation.
      *   **Update Database Models (`backend/app/db/models.py`)**
          *   Create a new `Summary` model (SQLModel) with fields: `id`, `document_id`, `user_id`, `summary_text`, `generated_at`, `ai_model`, `feedback`.
          *   Update the `Document` model's `status` field to include new states ('summarizing', 'summarized', 'summary-failed').
      *   **Configure Environment Variables (`.env` for backend)**
          *   Add `GEMINI_API_KEY` and any other relevant AI configuration.

      ### Testing

      *   **Unit Tests (Backend `pytest`)**:
          *   Test `gemini_client` functions for successful API calls and error handling.
          *   Test prompt construction in `summary_generator`.
          *   Test `Summary` model creation and persistence.
          *   Test `Document` status updates.
      *   **Integration Tests (Backend `pytest`)**:
          *   Test end-to-end flow from text extraction (mocking Story 3.2 output) to summary generation via Gemini API.
          *   Verify status updates in the `documents` table and `summaries` table population.
          *   Test API rate limit handling and recovery.
      *   **Performance Testing**:
          *   Measure the time taken for summary generation (API call + processing) for various text lengths to ensure it meets the 30-second threshold.
      *   **Manual/AI-Assisted Content Review**:
          *   Manually review generated summaries for accuracy, conciseness, and adherence to ethical AI guidelines. This will require human judgment or sophisticated AI-assisted review tools.
    </tasks>
  </story>

  <acceptanceCriteria>
    1.  **FR-CIP-2**: When text content from an uploaded document is available (from Story 3.2), the system allows a user to initiate summary generation.
    2.  **FR-CIP-2**: Upon initiation, the backend makes an API call to Gemini 2.5 with the extracted text and an optimized prompt for summarization.
    3.  **FR-CIP-2**: Gemini 2.5 returns a concise summary that accurately reflects the main concepts of the provided text.
    4.  **FR-CIP-2**: The summary generation process (from initiation to completion of the API call) is completed within 30 seconds for typical document sizes.
    5.  **FR-CIP-2**: The generated summary text is securely stored in the newly created `summaries` table, associated with the original `document_id` and `user_id`.
    6.  **FR-CIP-2**: The `status` field of the corresponding `documents` table entry is updated to 'summarized' upon successful summary generation.
    7.  **FR-CIP-2**: If the Gemini 2.5 API call fails, or returns an empty/invalid summary, the `documents` table `status` is updated to 'summary-failed', and relevant error logs are recorded.
    8.  **FR-CIP-2**: The system handles Gemini 2.5 API rate limits and potential service unavailability gracefully, providing appropriate feedback/retries.
    9.  **FR-CIP-2**: The AI prompt used for summarization adheres to ethical AI guidelines, aiming to minimize bias and ensure factual accuracy.
    10. **FR-CIP-2**: Frontend can query the status of summary generation for a given `document_id` and receive 'processing', 'summarized', or 'summary-failed'.
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>2. Content Ingestion &amp; Processing (FR-CIP)</section>
        <snippet>FR-CIP-2: AI Summary Generation: The system shall generate a concise summary from uploaded lecture notes using AI (Gemini 2.5), enabling users to quickly grasp key concepts. Ethical AI guidelines must be applied.</snippet>
      </artifact>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>Web App Specific Requirements</section>
        <snippet>AI Integration: Gemini 2.5 Pro/Flash will be central to AI-generated summaries, quizzes, and optional feedback, with careful prompt design, error handling, and cost management.</snippet>
      </artifact>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>Domain-Specific Requirements</section>
        <snippet>Ethical AI Use: Ethical considerations around bias in summaries or quizzes, fairness in grading, and transparency of AI operation will be paramount.</snippet>
      </artifact>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>Innovation &amp; Novel Patterns</section>
        <snippet>Seamless Personalization: Direct transformation of individual lecture notes into bespoke summaries and interactive quizzes.</snippet>
      </artifact>
      <artifact>
        <path>docs/PRD.md</path>
        <title>ibe160 - Product Requirements Document</title>
        <section>Edge Cases and Special Scenarios</section>
        <snippet>AI Service Unavailability: If the AI service (Gemini 2.5) is unavailable or returns an error, the user should be notified with a user-friendly message.</snippet>
      </artifact>
      <artifact>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Epic to Architecture Mapping</section>
        <snippet>Epic 3: Content Ingestion &amp; AI Summarization -&gt; backend/app/api/summaries/, backend/app/services/ai_generation/, backend/app/db/, backend/app/schemas/.</snippet>
      </artifact>
      <artifact>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Technology Stack Details</section>
        <snippet>AI Model: Gemini 2.5 Pro/Flash. Integration Points: Backend to Gemini API: Direct API calls to Gemini 2.5.</snippet>
      </artifact>
      <artifact>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Performance Considerations</section>
        <snippet>Asynchronous Operations: FastAPI's async capabilities for I/O-bound tasks. Background Jobs: For AI generation.</snippet>
      </artifact>
      <artifact>
        <path>docs/epics.md</path>
        <title>ibe160 - Epic Breakdown</title>
        <section>Epic 3: Content Ingestion &amp; AI Summarization</section>
        <snippet>Goal: Allow users to upload lecture notes and receive AI-generated summaries. Scope: Integration with Gemini 2.5 for summarization, displaying the summary in the UI.</snippet>
      </artifact>
    </docs>
    <code>
      <artifact>
        <path>backend/app/services/ai_generation/gemini_client.py</path>
        <kind>Python Service</kind>
        <symbol>call_gemini_summarize</symbol>
        <reason>New module for interacting with the Gemini 2.5 API to send requests and handle responses.</reason>
      </artifact>
      <artifact>
        <path>backend/app/services/ai_generation/summary_generator.py</path>
        <kind>Python Service</kind>
        <symbol>generate_summary</symbol>
        <reason>New module for orchestrating summary generation: prompt construction, Gemini API call, storing summary, and updating document status.</reason>
      </artifact>
      <artifact>
        <path>backend/app/api/summaries/main.py</path>
        <kind>FastAPI Endpoint</kind>
        <symbol>POST /api/v1/summaries/generate</symbol>
        <reason>New endpoint to trigger AI summary generation for a given document.</reason>
      </artifact>
      <artifact>
        <path>backend/app/api/summaries/main.py</path>
        <kind>FastAPI Endpoint</kind>
        <symbol>GET /api/v1/documents/{document_id}/status</symbol>
        <reason>New endpoint to allow frontend to query the current status of summary generation.</reason>
      </artifact>
      <artifact>
        <path>backend/app/db/models.py</path>
        <kind>SQLModel</kind>
        <symbol>Summary, Document</symbol>
        <reason>Create new Summary model and update Document model's status field to track generated summaries and document processing status.</reason>
      </artifact>
      <artifact>
        <path>backend/app/core/config.py</path>
        <kind>Python Configuration</kind>
        <symbol>GEMINI_API_KEY</symbol>
        <reason>Configure environment variables for Gemini API key and other AI-related settings securely.</reason>
      </artifact>
    </code>
    <dependencies>
      <nodejs>
        <package name="@hookform/resolvers" version="^5.2.2"/>
        <package name="@next/bundle-analyzer" version="^14.0.4"/>
        <package name="@radix-ui/react-dialog" version="^1.1.15"/>
        <package name="@radix-ui/react-dropdown-menu" version="^2.0.6"/>
        <package name="@radix-ui/react-icons" version="^1.3.0"/>
        <package name="@radix-ui/react-label" version="^2.1.8"/>
        <package name="@radix-ui/react-slot" version="^1.0.2"/>
        <package name="@radix-ui/react-toast" version="^1.2.15"/>
        <package name="@supabase/ssr" version="^0.1.0"/>
        <package name="@supabase/supabase-js" version="^2.39.3"/>
        <package name="@tanstack/react-query" version="^5.17.15"/>
        <package name="@tanstack/react-query-devtools" version="^5.17.15"/>
        <package name="@vercel/analytics" version="^1.1.1"/>
        <package name="axios" version="^1.6.5"/>
        <package name="class-variance-authority" version="^0.7.0"/>
        <package name="clsx" version="^2.1.0"/>
        <package name="geist" version="^1.0.0"/>
        <package name="lucide-react" version="^0.304.0"/>
        <package name="next" version="^14.1.0"/>
        <package name="next-themes" version="^0.2.1"/>
        <package name="nextjs-toploader" version="^1.6.4"/>
        <package name="react" version="18.2.0"/>
        <package name="react-dom" version="18.2.0"/>
        <package name="react-hook-form" version="^7.67.0"/>
        <package name="tailwind-merge" version="^2.2.0"/>
        <package name="undici" version="^5.28.2"/>
        <package name="zod" version="^4.1.13"/>
      </nodejs>
      <python>
        <package name="fastapi" version="^0.122.0"/>
        <package name="uvicorn" version="^0.38.0"/>
        <package name="supabase" version="^2.24.0"/>
        <package name="python-dotenv" version="^1.2.1"/>
        <package name="email-validator" version="^2.1.1"/>
        <package name="pypdf" version="^X.X.X"/>
        <package name="python-docx" version="^X.X.X"/>
        <package name="google-generativeai" version="^X.X.X"/>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Summary generation must be handled asynchronously to prevent API blocking, potentially using FastAPI BackgroundTasks or a dedicated queuing system.</constraint>
    <constraint>Robust error handling is required for Gemini API failures, rate limits, and unexpected API responses, with retry mechanisms (e.g., exponential backoff).</constraint>
    <constraint>Careful prompt engineering for Gemini 2.5 is essential to ensure accurate, concise, and unbiased summaries, adhering to ethical AI guidelines.</constraint>
    <constraint>The GEMINI_API_KEY and other sensitive AI configurations must be stored securely in environment variables.</constraint>
    <constraint>The summary generation process should complete within 30 seconds for typical document sizes.</constraint>
  </constraints>
  <interfaces>
    <interface>
      <name>Gemini 2.5 Summarization API</name>
      <kind>External API</kind>
      <signature>Google Gemini 2.5 API (specific endpoint and payload for text summarization)</signature>
      <path>backend/app/services/ai_generation/gemini_client.py</path>
    </interface>
    <interface>
      <name>Summary Generation Internal Call</name>
      <kind>Internal Function Call</kind>
      <signature>summary_generator.generate_summary(document_id: UUID, user_id: UUID, extracted_text: str)</signature>
      <path>backend/app/services/ai_generation/summary_generator.py</path>
    </interface>
    <interface>
      <name>Document Status Query API</name>
      <kind>REST Endpoint</kind>
      <signature>GET /api/v1/documents/{document_id}/status</signature>
      <path>backend/app/api/summaries/main.py</path>
    </interface>

  <tests>
    <standards>Unit, Integration, and Performance tests are required for the backend using pytest. Additionally, manual or AI-assisted content review is critical to ensure adherence to ethical AI guidelines for summary quality.</standards>
    <locations>
      <location>backend/tests/</location>
    </locations>
    <ideas>
      <idea acId="FR-CIP-2">Test successful Gemini API calls and robust error handling within the gemini_client.</idea>
      <idea acId="FR-CIP-2">Verify prompt construction in summary_generator ensures accurate and concise summarization.</idea>
      <idea acId="FR-CIP-2">Test proper creation and persistence of the Summary model, associating it correctly with document_id and user_id.</idea>
      <idea acId="FR-CIP-2">Validate Document status updates (e.g., 'summarizing', 'summarized', 'summary-failed') throughout the generation process.</idea>
      <idea acId="FR-CIP-2">Conduct end-to-end integration tests from text extraction (mocked) to summary generation via the Gemini API, verifying data flow and status changes.</idea>
      <idea acId="FR-CIP-2">Test graceful handling of Gemini API rate limits, service unavailability, and failures, including retry mechanisms with exponential backoff.</idea>
      <idea acId="FR-CIP-2">Measure summary generation time for various text lengths to ensure it meets the 30-second performance threshold.</idea>
      <idea acId="FR-CIP-2">Perform manual/AI-assisted review of generated summaries for accuracy, conciseness, and ethical AI compliance.</idea>
      <idea acId="FR-CIP-2">Test GET /api/v1/documents/{document_id}/status endpoint returns correct and timely status updates.</idea>
    </ideas>
  </tests>
</story-context>